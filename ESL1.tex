\documentclass{beamer}
\usetheme{Madrid}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[document]{ragged2e}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{float}
\usepackage{subfigure}
\usepackage[round]{natbib}

\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}

\title{Elements of statistical learning:
Chapter 2}
\centering
\begin{document}
\maketitle

\begin{frame}{Content}
\tableofcontents
\end{frame}

\section{Introduction}

\begin{frame}{Introduction}
\begin{block}{Supervised learning}
Goal is to use \textit{inputs} to predict \textit{outputs}.
\end{block}
\begin{itemize}
\item{inputs} are also referred to as \textit{predictors}, \textit{features} or \textit{independent variable}
\item{outputs} are also referred to as \textit{response} or \textit{dependent variables}
\end{itemize}
\medskip
The outputs may be

\begin{itemize}
\item{} quantitative (takes values in $\mathbb{R}$)
\item{} qualitative (also known as categorical or discrete)
\begin{itemize}
\item{} Ordered (e.g. small, medium or large)
\item{} Unordered (e.g. pass or fail, on or off)
\end{itemize}
\end{itemize}
\begin{block}{Regression vs classification}
We use \textit{regression} to predict quantitative outputs and \textit{classification} to predict qualitative outputs.
\end{block}
\end{frame}
\begin{frame}{Notations}
For different predictors $\{X_k\}_{k=1}^p$ across different observations $i=1,\cdots,n$ denote
\begin{eqnarray*}
 &X_{i,k}&\text{ for random variable and }x_{i,k}\text{ for an observation}\\
&X_i&\text{ for a $p\times 1$ vector of variables } - i.e. X_i=[X_{i,1},\cdots,X_{i,p}]'\\
&\bm{X}&\text{ for a $N\times p$ matrix of variables across different obvs}
\end{eqnarray*}
In other words
\[
\bm{X}=
\begin{bmatrix}
X_{11}&\cdots &X{1p}\\
X_{21}&\cdots &X{2p}\\
\vdots&\ddots&\vdots\\
X_{n1}&\cdots&X_{np}
\end{bmatrix}
=
\begin{bmatrix}
X_{1}'\\
X_{2}'\\
\vdots\\
X_{n}'
\end{bmatrix}
\]
Therefore, denotes $(X_i,Y_i)$ as the random variables and $(x_i,y_i)$ as the observed values at $i$.
\end{frame}
\section{Linear models and least squares}
\subsection{Linear regression}
\begin{frame}{Linear models and least squares}
To predict $Y$ (which would be denoted by $\hat{Y}$), we use the linear regression model, which may be expressed as
\begin{equation}\label{eq: LS1}
Y_i=\beta_0+\beta_1 X_{i,1}+\cdots+\beta_p X_{i,p}+\epsilon_i,\quad i=1,\cdots,n
\end{equation}
or as
\begin{equation}\label{eq: LS2}
Y_i=\beta_0+\sum\limits_{k=1}^P\beta_k X_{i,k}+\epsilon_i,\quad i=1,\cdots,n
\end{equation}
or as
\begin{equation}\label{eq: LS3}
Y_i=X_i'\beta+\epsilon_i,\quad,i=1,\cdots,n
\end{equation}
where $X_i=[1,X_{i,1},\cdots,X_{i,p}]'$ and $\beta=[\beta_0,\beta_1,\cdots,\beta_p]'$ are $(p+1)\times 1$ vectors.
\end{frame}

\begin{frame}
In matrix notation, the above can be expressed as
\begin{equation}\label{eq: LS4}
\bm{Y}=\bm{X}\beta+\bm{\epsilon}
\end{equation}
which if expanded is expressed as follows
\begin{equation}
\begin{bmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_n
\end{bmatrix}=
\begin{bmatrix}
1&X_{1,1}&X_{1,2}&\cdots&X_{1,p}\\
1&X_{2,1}&X_{2,2}&\cdots&X_{2,p}\\
\vdots&\vdots&\ddots&\ddots&\vdots\\
1&X_{n,1}&X_{n,2}&\cdots&X_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0\\
\beta_1\\
\vdots\\
\beta_p
\end{bmatrix}+
\begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\vdots\\
\epsilon_n
\end{bmatrix}
\end{equation}
\end{frame}

\begin{frame}{Least squares estimation}
The least squares estimation in one approach to fit the model. In essence, we find the coefficiens $\beta$ that minimize the sum of squared residuals.
Thus, we wish to minimize $RSS(\beta)$
\[
\arg \min_{\beta} RSS(\beta)\text{ or }(\bm{y}-\bm{X\beta})'(\bm{y}-\bm{X\beta})
\]
The above can be rearranged and expanded to (which is not necessary, as chain rule can be used)
\begin{eqnarray*}
(\bm{y}-\bm{X\beta})'(\bm{y}-\bm{X\beta})&=&(\bm{y}'-\bm{\beta' X'})(\bm{y}-\bm{X\beta})\\
&=&\bm{y}'\bm{y}-\bm{y}'\bm{X\beta}-\bm{\beta' X'}\bm{y}+\bm{\beta' X'}\bm{X\beta}
\end{eqnarray*}
\end{frame}

\begin{frame}
Differentiating with respect to $\beta$ yields
\begin{eqnarray*}
\frac{\partial RSS(\beta)}{\partial \beta}&=&0-\bm{X'y}-\bm{X'y}+2\beta \bm{X'X}\\
&=&-2\bm{X'y}+2\beta \bm{X'X}
\end{eqnarray*}
and as this is a minimization problem 
\begin{eqnarray*}
\frac{\partial RSS(\beta)}{\partial \beta}&=&0\\
-2\bm{X'y}+2\beta \bm{X'X}&=&0\\
\beta \bm{X'X}&=&\bm{X'y}
\end{eqnarray*}
and thus so long as $\bm{X'X}$ is non-singular, the LS estimator of $\hat{\beta}$ is
\begin{equation}
\hat{\beta}=(\bm{X'X})^{-1}\bm{X'y}
\end{equation}
\end{frame}
\subsection{Linear classification}
\begin{frame}{Logit models}
Now let us consider the case , where the dependent variable $Y_i$ can assume only two categories (say win or lose), and hence two discrete values (i.e. $Y_i=0$ or $Y_i=1$), where as the vector of independent variables are continuous, say $X_i\in\mathbb{R}^p$.

In order to restrict $Y_i$ to $0$ and $1$. In this case it would make sense to make the probability of $Y_i=1$ and not the value of $Y_i$ itself.  This leads to a probability model, which specifies the the probbility of the outcome as a function of the predictor:
\begin{eqnarray}
P[Y_i=1]&=&P[X_i,\beta]\\
P[Y_i=0]&=&1-P[X_i,\beta]
\end{eqnarray}
Since $P$ is a probability, it is bounded between $0$ and $1$. The regression equation may be revived by briefly denoting
\[
P(X_i,\beta)=X_i'\beta
\]
\end{frame}

\begin{frame}
As we wish the pobability to vary monotically with $X$, we may use a \textit{sigmoid} function:
\begin{equation}
P(X_i,\beta)=\frac{\exp(\beta'X_i)}{1+\exp(\beta'X_i)}
\end{equation}
Let us denote $Z_i=\beta'X_i$, then
\[
\lim_{z\rightarrow\infty}\frac{\exp(z)}{1+\exp(z)}=1
\]
and
\[
\lim_{z\rightarrow-\infty}\frac{\exp(z)}{1+\exp(z)}=0
\]
\end{frame}

\begin{frame}
Therefore,
\[
P[Y_i=1]=\frac{\exp(\beta'X)}{1+\exp(\beta'X)}
\]
and
\[
P[Y_i=0]=\frac{1}{1+\exp(\beta'X)}
\]
Alternatively, one could look at the odd $P[Y_i=1]/P[Y_i=0]$, which may be expressed as
\begin{eqnarray*}
\frac{P[Y_i=1]}{P[Y_i=0]}&=&\frac{\exp(\beta'X)}{1+\exp(\beta'X)}[1+\exp(\beta'X)].\\
&=&\exp(\beta'X).
\end{eqnarray*}
now taking the logarithm from both sides will yield
\begin{equation}
\log(odds)=\beta'X
\end{equation}
where now $\log(odds)$ is no longer bounded by $0$ and $1$.
\end{frame}
\section{Nearet-neigbour algorithm}
\begin{frame}{Nearet-neighbour algorithm}
\begin{itemize}
\item{} A non-parametric approach used for both \textit{classification} and \textit{regression}
\item{} Input consists of the $k$ closest training examples in the feature space.
\item{} Output depends on whether $K-NN$ is used or classification or regression.
\item{} For classification, the output is a class membership
\item{} For regression, this value is the average of the $k$ nearest neighbbours
\item{} Specifically it can be defined as
\begin{equation}
\hat{Y}(x)=\frac{1}{k}\sum\limits_{x_i\in N_k(x)}y_i,
\end{equation}
where $N_k(x)$ is the neighbourhood of $x$ defined by the $k$ closest points $x_i$ in the training sample.
\item{} In other words, find the $k$ nearest neighbours with $x_i$ closest to $x$, and average their responses $y_i$.
\end{itemize}
\end{frame}
\section{Statistical decision theory}
\subsection{L2 loss function}
\begin{frame}{L2 loss function}
\begin{itemize}
\item{} Let random variables $X\in \mathbb{R}^p$ and $Y\in\mathbb{R}$ with joint distribution function $F(x,y)$.
\item{} A function $g(X)$ is sought after for predicting $Y$, given values of the input $X$.
\item{} This theory requires a loss function $L(Y,g(X))$ for penalizing errors in prediction.
\item{} The most common and convenient is squared error loss 
\begin{equation}
L(Y,g(X))=(Y-g(X))^2
\end{equation}
which gives us the following criterion
\begin{equation}\label{eq: EPE}
EPE=\mathbb{E}[(Y-g(X))^2]
\end{equation}
\end{itemize}
\end{frame}

\begin{frame}{EXTRA: Some probability recap}
The expectation operator $\mathbb{E}$ is defined for discrete and continuous variables as follows
\begin{itemize}
\item{} Discrete variables:
\[
\mathbb{E}(X)=\sum\limits_{i\in k}p_ix_i
\]
where $k$ are the number of categories.
E.g. A coin has two possible states of head (quantified as $1$) and tail (quantified as $0$) with equal probability. Therefore, the expectated value of the outcome of a coin toss is
\[
\E(X)=0.5\times 0 +0.5\times 1=0.5
\]
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item{} Continuous variables:
\[
\E[X]=\int_{\R}xf(x)dx
\]
if density $f(x)$ exists. Otherwise,
\[
\E[X]=\int_{\R}xdF(x)
\]
(noting that $dF(X)/dx=f(x))$
\item{} Multivatiate continuous variable
\[
\E[g(X_1,\cdots,X_n)]=\int_{x_1}\cdots\int_{x_n} g(x_1,\cdots,x_n)dF(x_1,\cdots,x_n)
\]
and where the density $f(x_1,\cdots,x_n)$ exists
\[
\E[g(X_1,\cdots,X_n)]=\int_{x_1}\cdots\int_{x_n} g(x_1,\cdots,x_n)f(x_1,\cdots,x_n)dx_1\cdots dx_n
\]
\end{itemize}
\end{frame}


\begin{frame}
Finally, note that 
\begin{eqnarray*}
F(x,y)=F(x\mid y)F(y)\\
F(x,y)=F(y\mid x)F(x)
\end{eqnarray*}
Therefore, returning to (\ref{eq: EPE})
\begin{eqnarray*}
\E[(Y-g(X))^2]&=&\int_x\int_y(y-g(x))^2dF(y,x)\\
&=&\int_x\int_y(y-g(x))^2f(y,x)dydx\\
&=&\int_x\int_y(y-g(x))^2f(y\mid x)f(x)dydx\\
&=&\int_x\E_{y\mid x}[(Y-g(X))^2\mid X]f(x)dx\\
&=&\E_X\E_{Y\mid X}[(Y-g(X))^2\mid X]
\end{eqnarray*}
\end{frame}

\begin{frame}
\begin{itemize}
\item{} As the above expression is conditioned on $X$, there is no longer any dependency between $X$ and the function $g$.

\item{} furthermore, $[Y-g]^2$ is a convex function and we may minimize to solve for $g$
\begin{eqnarray*}
g(x)&=&\arg\min_{g}\E_{y\mid x}[(Y-g(X))^2\mid X=x]\\
&\rightarrow& \frac{\partial}{\partial g}\int[y-g]^2 f(y\mid x)dy=0\\
&\rightarrow& \int \frac{\partial}{\partial g}[y-g]^2 f(y\mid x)dy=0\\
&\rightarrow& -2\int[y-g]  f(y\mid x)dy=0\\
&\rightarrow& 2g\int f(y\mid x)dy=2\int y  f(y\mid x)dy\\
&\rightarrow& 2g\int f(y\mid x)dy=2\E_{Y\mid X}[Y\mid X=x]
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}
Note that 
\[
\int_{\R}f(y\mid x)dy=1
\]
\begin{itemize}
\item{}Thus
\[
\E[Y\mid X=x]=g(x)
\]
\item{} This conditional function is also known as the regression function. Thus, the best predictor of $Y$ at any point $X=x$ is the conditional mean.
\end{itemize}
\end{frame}
\subsection{L1 loss function}
\begin{frame}{L1 loss function}
\begin{itemize}
\item{} The L2 loss function is analytically more desirable, but an L1 criteria of the sort
\[
\E[\lvert Y-g(X)\lvert]
\]
is more robust to outliers. We may express (\ref{eq: EPE}) in the following manner using the L1 criteria:
\begin{eqnarray*}
\E[\lvert Y-g(X)\lvert]=\int\int \lvert Y-g(X)\lvert f(x,y)dydx
\end{eqnarray*}
where as before it may be expressed as
\[
\E_X\E_{Y\mid X}[\lvert Y-g(X)\lvert\mid X]
\]
and minimized by differentiating with respect to $g$ - i.e.
\[
\frac{\partial}{\partial g}\int\lvert y-g(x)\lvert f(y\mid x)dy=0
\]
\end{itemize}
\end{frame}

\begin{frame}
The latter can be approxmiated by
\begin{eqnarray}\label{eq: l1}
\begin{split}
\frac{\partial}{\partial g}\int\lvert y-g\lvert f(y\mid x)dy&=&\frac{\partial}{\partial g}\lim_{n\rightarrow \infty}\frac{1}{n}\sum\limits_{i=1}^n\lvert y_i-g\lvert\\
&\approx&\frac{\partial}{\partial g}\frac{1}{n}\sum\limits_{i=1}^{n}\lvert y_i-g\lvert
\end{split}
\end{eqnarray}
Note that the absolute function is piecewise 
\[
\lvert y_i-g\lvert=
\begin{cases}
 y_i-g,\quad y_i-g> 0\\
g-y_i,\quad y_i-g< 0\\
0,\quad y_i=g
\end{cases}
\]
\end{frame}

\begin{frame}
So that taking the derivative is not continuous at zero
\[
\frac{\partial}{\partial g}\lvert y_i-g\lvert=
\begin{cases}
 -1,\quad y_i-g> 0\\
1,\quad y_i-g< 0\\
0,\quad y_i=g
\end{cases}
\]
which is very similar to a sign function - i.e.
\[
sgn(z)=
\begin{cases}
 -1,\quad z< 0\\
1,\quad z>0\\
0,\quad z=0
\end{cases}
\]
which implies that we may express (\ref{eq: l1}) as
\begin{eqnarray*}
\frac{\partial}{\partial g}\int\lvert y-g\lvert f(y\mid x)dy&=&0\\
\frac{1}{n}\sum\limits_{i=1}^{n}-sgn(y_i-g)&=&0\\
\sum\limits_{i=1}^{n}sgn(y_i-g)&=&0
\end{eqnarray*}
\end{frame}
\section{MSE}
\begin{frame}
The mean squared error at a nominated test point $x_0$ for a \textit{deterministic} model of the form
\[
Y=f(X)=\exp(-8\lvert\lvert X\rvert\rvert^2)
\]
without any measurement errors, can be obtained by
\begin{eqnarray*}
MSE(x_0)&=&\E_{T}[(f(x_0)-\hat{y}_0)^2]\\
&=&\E_{T}[(f(x_0)\underbrace{-\E_T(\hat{y}_0)+\E_T(\hat{y}_0)}_{\text{add and deduct}}-\hat{y}_0)^2]
\end{eqnarray*}
Noting that by noting $\underbrace{f(x_0)-\E_T(\hat{y}_0)}_{a}$ and $\underbrace{\E_T(\hat{y}_0)-\hat{y}_0}_{a}$, the above is equivalent to $\E_T[(a+b)^2]$, and such can be expanded as follows
\begin{eqnarray*}
MSE(x_0)&=&\E_{T}[(f(x_0)-\E_T(\hat{y}_0))^2+(\E_T(\hat{y}_0)-\hat{y}_0)^2\\
&+&2[(f(x_0)-\E_T(\hat{y}_0))(\E_T(\hat{y}_0)-\hat{y}_0)]
\end{eqnarray*}
\end{frame}

\begin{frame}
which may further get expanded as
\begin{eqnarray*}
MSE(x_0)&=&\E_{T}[(f(x_0)-\E_T(\hat{y}_0))^2+(\E_T(\hat{y}_0)-\hat{y}_0)^2\\
&+&2f(x_0)\E_T(\hat{y}_0)-2f(x_0)\hat{y}_0-2\E_T(\hat{y}_0)\E_T(\hat{y}_0)+2\E_T(\hat{y}_0)\hat{y}_0]\\
&=&\E_{T}[(f(x_0)-\E_T(\hat{y}_0))^2]+\E_T[(\E_T(\hat{y}_0)-\hat{y}_0)^2]\\
&+&2f(x_0)^2-2f(x_0)\hat{y}_0-2f(x_0)^2+2f(x_0)\hat{y}_0\\
&=&\E_{T}[(f(x_0)-\E_T(\hat{y}_0))^2]+\E_T[(\E_T(\hat{y}_0)-\hat{y}_0)^2]
\end{eqnarray*}
Note that by definition
\begin{itemize}
\item{} Variance: $\E_{T}[(f(x_0)-\E_T(\hat{y}_0))^2]$; and
\item{} Bias: $\E_T[(\E_T(\hat{y}_0)-\hat{y}_0)]$
\end{itemize}
Thus,
\begin{equation}
MSE(x_0)=\sigma^2_T(\hat{y}_0)+Bias^2(\hat{y}_0)
\end{equation}
\end{frame}

\section{Proofs of eq 2.26 and 2.27}
\begin{frame}{Proofs of eq 2.26 and 2.27}
Assume we know that the relationship between $X$ and $Y$ linear and follows the relationship
\begin{equation}\label{eq: model}
Y_i=X_i'\beta+\epsilon_i,\quad \epsilon_i\sim N(0,\sigma^2)
\end{equation}
and the model is fitted using the LS estimator, which is
\[
\hat{\beta}=(X'X)^{-1}X'Y
\]
which may alternatively be expressed as
\begin{eqnarray*}
\hat{\beta}=(X'X)^{-1}X'(X\beta+\epsilon)&=&(X'X)^{-1}(X'X)\beta+(X'X)^{-1}X'\epsilon\\
&=&\beta+(X'X)^{-1}X'\epsilon
\end{eqnarray*}
\end{frame}

\begin{frame}
Thus, the linear model (\ref{eq: model}) can be expressed as
\begin{eqnarray*}
Y_i=X_i'[\beta+(X'X)^{-1}X'\epsilon]&=&X_i'\beta+X_i'(X'X)^{-1}X'\epsilon\\
&=&X_i'\beta+X(X'X)^{-1}X_i\epsilon
\end{eqnarray*}
Thus, at a nominated test point $x_0$, we have
\begin{eqnarray*}
\hat{y}_0&=&x_0'\beta+X(X'X)^{-1}x_0\epsilon\\
\hat{y}_0&=&x_0'\beta+\sum\limits_{i=1}^{N}l_i(x_0)\epsilon_i
\end{eqnarray*}
where $l_i(x_0)$ is the $i-$th elemenet of the $N\time1$ vector $X(X'X)^{-1}x_0$.
\end{frame}
\begin{frame}{Proof of equation 2.27}
Here, since our target is no longer deterministic (pay attention to the error term in (\ref{eq: model}), the expected prediction error can be written as 
\begin{equation}
EPE(x_0)=\E_{y_0\mid x_0}\E_T(y_0-\hat{y}_0)^2
\end{equation}
which as before may get expanded to
\begin{eqnarray*}
EPE(x_0)&=&\E_{y_0\mid x_0}\E_T[(y_0-g(x_0))+(g(x_0)-\hat{y}_0)]^2\\
&=&\E_{y_0\mid x_0}\E_T[(y_0-g(x_0))^2]\\
&+&2\E_{y_0\mid x_0}\E_T[(y_0-g(x_0))(g(x_0)-\hat{y}_0)]\\
&+&\E_{y_0\mid x_0}\E_T[(g(x_0)-\hat{y}_0)^2]
\end{eqnarray*} 
\end{frame}
\begin{frame}
The first term $\E_{y_0\mid x_0}\E_T[(y_0-g(x_0))^2]$ is independent of the training set and as such can be expressed as 
\begin{eqnarray*}
\E_{y_0\mid x_0}\E_T[(y_0-g(x_0))^2]&=&\E_{y_0\mid x_0}[(y_0-g(x_0))^2]\\
&=&\int (y_0-g(x_0))^2 f(y\mid x)dy
\end{eqnarray*}
and as $\E(y_0)=g(x_0)$ the first term is nothing but the conditional variance $\sigma^2$.

The second term can be factorized as before  to get
\begin{eqnarray*}
\E_{y_0\mid x_0}\E_T[(y_0-g(x_0))(g(x_0)-\hat{y}_0)]&=&\E_{y_0\mid x_0}\E_T[y_0g(x_0)]-\E_{y_0\mid x_0}\E_T[y_0\hat{y}_0]\\
&-&\E_{y_0\mid x_0}\E_T[g(x_0)^2]\\
&+&\E_{y_0\mid x_0}\E_T[g(x_0)\hat{y}_0]
\end{eqnarray*}
\end{frame}

\begin{frame}
$\hat{y}_0$ is dependent on $T$ and $g(x_0)=\E[y_0]$, so it is a constant term that we can reduce above to
\[
=g(x_0)\E_{y_0\mid x_0}[y_0]-\E_{y_0\mid x_0}[y_0\E_T[\hat{y}_0]]-g(x_0)^2+g(x_0)\E_{y_0\mid x_0}\E_T[\hat{y}_0]
\]
noting that
\[
\E_{y\mid x_0}[y_0]=\int y_0 f(y_0\mid x_0)dy=g(x_0)
\]
and for the 2nd term in relationship
\[
\E_{y_0\mid x_0}[y_0\E_T[\hat{y}_0]]=\E_T[\hat{y}_0]\E_{y_0\mid x_0}[y_0]=\E_T[\hat{y}_0]g(x_0)
\]
and the last term we get
\[
g(x_0)\E_{y_0\mid x_0}\E_T[\hat{y}_0]=g(x_0)\E[\hat{y}_0]
\]
\end{frame}
\begin{frame}
Therefore, since $\E_{y_0\mid x_0}[y_0]=g(x_0)$
\begin{eqnarray*}
\E_{y_0\mid x_0}\E_T[(y_0-g(x_0))(g(x_0)-\hat{y}_0)]&=&g(x_0)\E_{y_0\mid x_0}[y_0]-\E_{y_0\mid x_0}[y_0\E_T[\hat{y}_0]]\\
&-&g(x_0)^2+g(x_0)\E_{y_0\mid x_0}\E_T[\hat{y}_0]\\
&=&g(x_0)^2-g(x_0)\E_{T}[\hat{y}_0]\\
&-&g(x_0)^2+g(x_0)\E_T[\hat{y}_0]=0
\end{eqnarray*}
Thus,
\begin{eqnarray*}
EPE(x_0)&=&\E_{y_0\mid x_0}\E_T[(y_0-g(x_0))^2]+\E_{y_0\mid x_0}\E_T[(g(x_0)-\hat{y}_0)^2]\\
&=&\sigma^2+\E_{y_0\mid x_0}\E_T[(g(x_0)-\hat{y}_0)^2]\\
&=&\sigma^2+\underbrace{\E_{y_0\mid x_0}\E_T[g(x_0)^2]-2\E_{y_0\mid x_0}\E_T[g(x_0)\hat{y}_0]}+\E_{y_0\mid x_0}\E_T[\hat{y}_0^2]
\end{eqnarray*}
Noting that
\begin{eqnarray*}
\E_{y_0\mid x_0}\E_T[(g(x_0)-\E_T[\hat{y}_0])^2]&=&\E_{y_0\mid x_0}\E_T[g(x_0)^2]-2\E_{y_0\mid x_0}\E_T[g(x_0)\hat{y}_0]\\
&+&\E_{y_0\mid x_0}\E_T[\hat{y}_0]^2
\end{eqnarray*}
\end{frame}
\begin{frame}
The expression in the brackets can be substituted with
\[
 \E_{y_0\mid x_0}\E_T[(g(x_0)-\E_T[\hat{y}_0])^2]-\E_{y_0\mid x_0}\E_T[\hat{y}_0]^2
\]
which implies
\begin{eqnarray*}
EPE(x_0)&=&\E_{y_0\mid x_0}\E_T[(y_0-g(x_0))^2]+\E_{y_0\mid x_0}\E_T[(g(x_0)-\hat{y}_0)^2]\\
&=&\sigma^2+\E_{y_0\mid x_0}\E_T[(g(x_0)-\E_T[\hat{y}_0])^2]\\
&-&\E_{y_0\mid x_0}\E_T[\hat{y}_0]^2+\E_{y_0\mid x_0}\E_T[\hat{y}_0^2]
\end{eqnarray*}
noting that
\[
\sigma^2_T(\hat{y}_0)=\E_T[\hat{y}_0^2]-\E_T[\hat{y}_0]^2
\]
Hence
\begin{eqnarray*}
EPE(x_0)&=&\sigma^2(y_0\mid x_0)+\E_{y_0\mid x_0}\E_T[(g(x_0)-\E_T[\hat{y}_0])^2]+\sigma_T^2(\hat{y}_0)\\
&=&\sigma^2(y_0\mid x_0)+Bias^2(\hat{y}_0)+\sigma_T^2(\hat{y}_0)
\end{eqnarray*}
\end{frame}
\section{Maximum likelihood estimation}
\begin{frame}{Maximum likelihood estimation}
For a random sample $\{y_i\}_{i=1}^{N}$ from a density function $P_{\theta}[y]$ indexed by some parameters $\theta$, the likelihood function is
\begin{equation}\label{eq: likelihood}
 L(\theta)=\prod\limits_{i=1}^{N}P_{\theta}[y_i]
\end{equation}
while the log-likelihood function is given by
\begin{equation}\label{eq: log likelihood}
 l(\theta)=\sum\limits_{i=1}^{N}P_{\theta}[y_i]
\end{equation}
Assuming with the additive error model is of the form
\[
Y=g_{\theta}(X)+\varepsilon,\quad \varepsilon\sim N(g_{\theta}(X),\sigma^2)
\]
then the log-likelihoof function of data is derived as follows:
\end{frame}
\begin{frame}
We know that the conditional normal distribution function is
\[
F(y\mid g_{\theta}(x))=\frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^{y}\exp\left(\frac{-(y-g_{\theta}(x))^2}{2\sigma^2}\right)
\]
with a density function
\[
f(y\mid g_{\theta}(x))=\frac{1}{\sigma\sqrt{2\pi}}\exp\left(\frac{-(y-g_{\theta}(x))^2}{2\sigma^2}\right)
\]
thus,
\begin{eqnarray*}
l(\theta)&=&\sum\limits_{i=1}^N\log\left(\frac{1}{\sigma\sqrt{2\pi}}\exp\left(\frac{-(y-g_{\theta}(x))^2}{2\sigma^2}\right)\right)\\
&=&N\log\left(\frac{1}{\sigma\sqrt{2\pi}}\right)+\sum\limits_{i=1}^N\log\left(\exp\left(\frac{-(y-g_{\theta}(x))^2}{2\sigma^2}\right)\right)\\
&=&-N\log(\sigma\sqrt{2\pi})+\sum\limits_{i=1}^N\left(\frac{-(y-g_{\theta}(x))^2}{2\sigma^2}\right)
\end{eqnarray*}
\end{frame}
\begin{frame}
\begin{eqnarray*}
l(\theta)&=&-N\log(\sigma\sqrt{2\pi})-\frac{1}{2\sigma^2}\sum\limits_{i=1}^N\left(y_i-g_{\theta}(x_i)\right)^2\\
&=&-N\log(\sigma)-N\log(\sqrt{2\pi})-\frac{1}{2\sigma^2}\sum\limits_{i=1}^N\left(y_i-g_{\theta}(x_i)\right)^2\\
&=&-N\log(\sigma)-\frac{N}{2}\log(2\pi)-\frac{1}{2\sigma^2}\sum\limits_{i=1}^N\left(y_i-g_{\theta}(x_i)\right)^2
\end{eqnarray*}
which is the proof of equation (2.35) in the book.
\end{frame}
%\bibliographystyle{apa}
%\bibliography{References_Final}
\end{document}
