\documentclass{beamer}
\usetheme{Madrid}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[document]{ragged2e}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{float}
\usepackage{subfigure}
\usepackage[round]{natbib}

\title{Elements of statistical learning:
Chapter 2}
\centering
\begin{document}
\maketitle

\begin{frame}{Content}
\tableofcontents
\end{frame}

\section{Introduction}

\begin{frame}{Introduction}
\begin{block}{Supervised learning}
Goal is to use \textit{inputs} to predict \textit{outputs}.
\end{block}
\begin{itemize}
\item{inputs} are also referred to as \textit{predictors}, \textit{features} or \textit{independent variable}
\item{outputs} are also referred to as \textit{response} or \textit{dependent variables}
\end{itemize}
\medskip
The outputs may be

\begin{itemize}
\item{} quantitative (takes values in $\mathbb{R}$)
\item{} qualitative (also known as categorical or discrete)
\begin{itemize}
\item{} Ordered (e.g. small, medium or large)
\item{} Unordered (e.g. pass or fail, on or off)
\end{itemize}
\end{itemize}
\begin{block}{Regression vs classification}
We use \textit{regression} to predict quantitative outputs and \textit{classification} to predict qualitative outputs.
\end{block}
\end{frame}
\begin{frame}{Notations}
For different predictors $\{X_k\}_{k=1}^p$ across different observations $i=1,\cdots,n$ denote
\begin{eqnarray*}
 &X_{i,k}&\text{ for random variable and }x_{i,k}\text{ for an observation}\\
&X_i&\text{ for a $p\times 1$ vector of variables } - i.e. X_i=[X_{i,1},\cdots,X_{i,p}]'\\
&\bm{X}&\text{ for a $N\times p$ matrix of variables across different obvs}
\end{eqnarray*}
In other words
\[
\bm{X}=
\begin{bmatrix}
X_{11}&\cdots &X{1p}\\
X_{21}&\cdots &X{2p}\\
\vdots&\ddots&\vdots\\
X_{n1}&\cdots&X_{np}
\end{bmatrix}
=
\begin{bmatrix}
X_{1}'\\
X_{2}'\\
\vdots\\
X_{n}'
\end{bmatrix}
\]
Therefore, denotes $(X_i,Y_i)$ as the random variables and $(x_i,y_i)$ as the observed values at $i$.
\end{frame}
\section{Linear models and least squares}
\subsection{Linear regression}
\begin{frame}{Linear models and least squares}
To predict $Y$ (which would be denoted by $\hat{Y}$), we use the linear regression model, which may be expressed as
\begin{equation}\label{eq: LS1}
Y_i=\beta_0+\beta_1 X_{i,1}+\cdots+\beta_p X_{i,p}+\epsilon_i,\quad i=1,\cdots,n
\end{equation}
or as
\begin{equation}\label{eq: LS2}
Y_i=\beta_0+\sum\limits_{k=1}^P\beta_k X_{i,k}+\epsilon_i,\quad i=1,\cdots,n
\end{equation}
or as
\begin{equation}\label{eq: LS3}
Y_i=X_i'\beta+\epsilon_i,\quad,i=1,\cdots,n
\end{equation}
where $X_i=[1,X_{i,1},\cdots,X_{i,p}]'$ and $\beta=[\beta_0,\beta_1,\cdots,\beta_p]'$ are $(p+1)\times 1$ vectors.
\end{frame}

\begin{frame}
In matrix notation, the above can be expressed as
\begin{equation}\label{eq: LS4}
\bm{Y}=\bm{X}\beta+\bm{\epsilon}
\end{equation}
which if expanded is expressed as follows
\begin{equation}
\begin{bmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_n
\end{bmatrix}=
\begin{bmatrix}
1&X_{1,1}&X_{1,2}&\cdots&X_{1,p}\\
1&X_{2,1}&X_{2,2}&\cdots&X_{2,p}\\
\vdots&\vdots&\ddots&\ddots&\vdots\\
1&X_{n,1}&X_{n,2}&\cdots&X_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0\\
\beta_1\\
\vdots\\
\beta_p
\end{bmatrix}+
\begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\vdots\\
\epsilon_n
\end{bmatrix}
\end{equation}
\end{frame}

\begin{frame}{Least squares estimation}
The least squares estimation in one approach to fit the model. In essence, we find the coefficiens $\beta$ that minimize the sum of squared residuals.
Thus, we wish to minimize $RSS(\beta)$
\[
\arg \min_{\beta} RSS(\beta)\text{ or }(\bm{y}-\bm{X\beta})'(\bm{y}-\bm{X\beta})
\]
The above can be rearranged and expanded to (which is not necessary, as chain rule can be used)
\begin{eqnarray*}
(\bm{y}-\bm{X\beta})'(\bm{y}-\bm{X\beta})&=&(\bm{y}'-\bm{\beta' X'})(\bm{y}-\bm{X\beta})\\
&=&\bm{y}'\bm{y}-\bm{y}'\bm{X\beta}-\bm{\beta' X'}\bm{y}+\bm{\beta' X'}\bm{X\beta}
\end{eqnarray*}
\end{frame}

\begin{frame}
Differentiating with respect to $\beta$ yields
\begin{eqnarray*}
\frac{\partial RSS(\beta)}{\partial \beta}&=&0-\bm{X'y}-\bm{X'y}+2\beta \bm{X'X}\\
&=&-2\bm{X'y}+2\beta \bm{X'X}
\end{eqnarray*}
and as this is a minimization problem 
\begin{eqnarray*}
\frac{\partial RSS(\beta)}{\partial \beta}&=&0\\
-2\bm{X'y}+2\beta \bm{X'X}&=&0\\
\beta \bm{X'X}&=&\bm{X'y}
\end{eqnarray*}
and thus so long as $\bm{X'X}$ is non-singular, the LS estimator of $\hat{\beta}$ is
\begin{equation}
\hat{\beta}=(\bm{X'X})^{-1}\bm{X'y}
\end{equation}
\end{frame}
\subsection{Linear classification}
\begin{frame}{Logit models}
Now let us consider the case , where the dependent variable $Y_i$ can assume only two categories (say win or lose), and hence two discrete values (i.e. $Y_i=0$ or $Y_i=1$), where as the vector of independent variables are continuous, say $X_i\in\mathbb{R}^p$.

In order to restrict $Y_i$ to $0$ and $1$. In this case it would make sense to make the probability of $Y_i=1$ and not the value of $Y_i$ itself.  This leads to a probability model, which specifies the the probbility of the outcome as a function of the predictor:
\begin{eqnarray}
P[Y_i=1]&=&P[X_i,\beta]\\
P[Y_i=0]&=&1-P[X_i,\beta]
\end{eqnarray}
Since $P$ is a probability, it is bounded between $0$ and $1$. The regression equation may be revived by briefly denoting
\[
P(X_i,\beta)=X_i'\beta
\]
\end{frame}

\begin{frame}
As we wish the pobability to vary monotically with $X$, we may use a \textit{sigmoid} function:
\begin{equation}
P(X_i,\beta)=\frac{\exp(\beta'X_i)}{1+\exp(\beta'X_i)}
\end{equation}
Let us denote $Z_i=\beta'X_i$, then
\[
\lim_{z\rightarrow\infty}\frac{\exp(z)}{1+\exp(z)}=1
\]
and
\[
\lim_{z\rightarrow-\infty}\frac{\exp(z)}{1+\exp(z)}=0
\]
\end{frame}

\begin{frame}
Therefore,
\[
P[Y_i=1]=\frac{\exp(\beta'X)}{1+\exp(\beta'X)}
\]
and
\[
P[Y_i=0]=\frac{1}{1+\exp(\beta'X)}
\]
Alternatively, one could look at the odd $P[Y_i=1]/P[Y_i=0]$, which may be expressed as
\begin{eqnarray*}
\frac{P[Y_i=1]}{P[Y_i=0]}&=&\frac{\exp(\beta'X)}{1+\exp(\beta'X)}[1+\exp(\beta'X)].\\
&=&\exp(\beta'X).
\end{eqnarray*}
now taking the logarithm from both sides will yield
\begin{equation}
\log(odds)=\beta'X
\end{equation}
where now $\log(odds)$ is no longer bounded by $0$ and $1$.
\end{frame}
\section{Nearet-neigbour algorithm}
\begin{frame}{Nearet-neighbour algorithm}
\begin{itemize}
\item{} A non-parametric approach used for both \textit{classification} and \textit{regression}
\item{} Input consists of the $k$ closest training examples in the feature space.
\item{} Output depends on whether $K-NN$ is used or classification or regression.
\item{} For classification, the output is a class membership
\item{} For regression, this value is the average of the $k$ nearest neighbbours
\end{itemize}
\end{frame}
\begin{frame}{Dependence modelling using copulas}
\begin{itemize}
\item {} As the mediangale assumption allows for non-linear serial dependence, testing assumption A1 by considering linear correlation is inappropriate. 

\item{} we suggest fitting copula models, which provide the means of separating the marginal distribution of the process from their respective dependence structure.
\item{} For instance, the marginals can be assumed to possess standard normal distributions, while the nonlinear dependency is modeled using rank correlation measures (e.g. Kendall Tau) and copulas, where the latter are invariant under monotonic transformations; hence, they are not affected by the marginal distributions [see ].
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item{}A special case is where $\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_{T-1},\varepsilon_T$ are distributed according to $N(0,1)$. As suggested before, since the form of the serial dependence of the errors is non-linear, we may estimate the bivariate probabilities using copulas. 

\item{}The latter stem from a Theorem brought forward by, which states that there is a copula $C$ such that
\[
F_{\varepsilon}(-\beta' x_0,\cdots,-\beta' x_{n-1})=C(F_1(-\beta' x_0),\cdots,F_n(-\beta' x_{n-1})).
\]
\item{}Therefore, as noted earlier, the bivariate probabilities can be calculated using copulas, which separate the marginal distribution of the error terms from their dependence structure. We may consider a rank correlation measure (e.g. Kendall Tau) as the measure of non-linear dependency and choose the Gaussian copula for evaluating the bivariate probabilities $P\left[ \text{ }\varepsilon_{t-1}<\cdot ,\-
\text{ }\varepsilon_{t}<\cdot \mid \mathcal{I}_{t-1}\right]$.
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item{} First let us forget about the Theorem mentioned earlier, where $s(\varepsilon_1),\cdots, s(\varepsilon_n)$ are i.i.d conditional on $\mathcal{I}_{t-1}$. Let us explore this from an earlier point and what led to this Theorem in . We know from the mediangale assumption that
\begin{align*}
\begin{array}{l}
P[s(\varepsilon_t)=s_t\mid s(\varepsilon_{t-1})=s_{t-1},\cdots,s(\varepsilon_{1})=s_{1}]=\\
P[\varepsilon_t\geq0\mid \varepsilon_{t-1},\cdots,\varepsilon_{1}]^{s(\varepsilon_t)}
P[\varepsilon_t<0\mid \varepsilon_{t-1},\cdots,\varepsilon_{1}]^{1-s(\varepsilon_t)}=\frac{1}{2}
\end{array}
\end{align*}
\item{} As this corresponds only to the median, it is only true \textit{iff} both sides of the inequality are zero. SInce the probabilities $P[\varepsilon_t\geq0\mid \varepsilon_{t-1},\cdots,\varepsilon_{1}]$ and $P[\varepsilon_t<0\mid \varepsilon_{t-1},\cdots,\varepsilon_{1}]$ are the same and equal to $1/2$ for all $t=1,\cdots,n$. Thus, it can be argued that $s(\varepsilon_1),\cdots,s(\varepsilon_n)$ which are defined by the aformentioned probabilties can be considered i.i.d, as their distribution is determined by the said probabilities, which are identical over time.
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item{} And here is the proposition (pay careful attention to the last paragraph):
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item{} Now let us instead assume that we are interested in the signs $\tilde{s}(\varepsilon+c_t)$, where $c_t$ for $t=1,\cdots,n$ are some constants. Then the likelihood function in terms of the signs $\tilde{s}_1,\cdots,\tilde{s}_n$ 
\begin{align*}
\begin{array}{l}
L(\tilde{s}_1,\cdots,\tilde{s}_n\mid X)=\\
\prod\limits_{t=1}^{n}P[\varepsilon_t\geq -c_t\mid \varepsilon_1,\cdots,\varepsilon_n]^{\tilde{s}_t}P[\varepsilon_t< -c_t \mid \varepsilon_1,\cdots,\varepsilon_n]^{1-\tilde{s}_t}	
\end{array}
\end{align*}
\item{} Now the joint p.m.f. depnends on all the past signs and and thus is no longer $i.i.d$, as it violates the Mediangale assumption (particularly the point about permutations mentioned in Proposition 1 of). In other words, we no longer have that the p.m.fs $P[\varepsilon_t<-c_t\mid \varepsilon_{t-1},\cdots,\varepsilon_{1}]$ are constant and identical over time, so the distribution of the signs $\tilde{s}(\varepsilon_1+c_1),\cdots,\tilde{s}(\varepsilon_n+c_n)$, now dependens on the joint p.m.fs.
\end{itemize}
\end{frame}

\begin{frame}{Alternative proof}
\begin{itemize}
\item{}Under the null hypotheis, it is evident that
\[
P[\varepsilon_t\geq0\mid X]=P[\varepsilon_t<0\mid X],\quad t=1,\cdots,n
\]
is equivalent to
\[
P[y_t\geq0\mid X]=P[y_t<0\mid X],\quad t=1,\cdots,n
\]
\item{}Therefore, the mediangale assumption can be extended to $y_1,\cdots,y_n$:
\[
P[y_t\geq0\mid y_1,\cdots,y_n,X]=P[y_t<0\mid y_1,\cdots,y_n,X]=\frac{1}{2}
\]
\end{itemize}
\end{frame}

\begin{frame}{Alternative proof}
\begin{itemize}
\item{}Then the variables $s(y_1),\cdots,s(y_n)$ are i.i.d conditional on $X$ according to the distribution
\[
P[s(y_t)=1\mid X]=P[s(y_t)=0\mid X]=\frac{1}{2},\quad t=1,\cdots,n
\]
\item{} Under the alternative hypothesis, however, the mediangale property does not hold for permutations $\pi:i\rightarrow j$ as noted in the Proposition 3.1 of , as the conditional distribution of the signs vary across observations.

\item{} Therefore, the signs $s(u_1),\cdots,s(y_n)$ can no longer be assumed to be i.i.d.
\end{itemize}
\end{frame}

%\bibliographystyle{apa}
%\bibliography{References_Final}
\end{document}
