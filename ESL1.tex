\documentclass{beamer}
\usetheme{Madrid}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{float}
\usepackage{subfigure}
\usepackage[round]{natbib}

\title{POS tests for predictive regressions}
\centering
\begin{document}
\maketitle
\begin{frame}{Content}

\begin{itemize}
\item Framework 
\item The test statistic in the presence of stochastic regressors 
\item The problem
\item Dependence modelling using copulas
\end{itemize}

\end{frame}
\begin{frame}{Framework}
\begin{itemize}
\item{} Predictive regressions assume the form
\begin{equation}
y_t=\beta x_{t-1}+\varepsilon_t\quad t=1,\cdots,n
\end{equation}
 \item{} $x_t$ is usually a highly persistent regressor such that the disturbances of the regressor are correlated with the error terms of the predictive regression. In a framework considered by, $x_t$ is assumed to follow a stationary AR($1$) process, such that
\[
x_t=\theta x_{t-1}+u_t
\]
where $(u_t,\varepsilon_t)$ are bivariate normal distribution, with the covariance matrix
\[
\begin{bmatrix}
1&\sigma_{\varepsilon u}\\
\sigma_{\varepsilon u}&\sigma^2_u
\end{bmatrix}
\] 
\item{} Therefore, $u_t$ and $\varepsilon_t$ are contemporaneously correlated with $corr(\varepsilon_t,u_t)\neq 0$, which implies that there is feedback from $u_t$ to $x_t$, through $\varepsilon_t$, and as such $corr(u_t,x_{t+j})\neq0$ for $j\geq 0$.
\end{itemize}
\end{frame}
\begin{frame}{POS-based tests}
\begin{itemize}
\item{}To build our POS-based tests in the context of predictive regressions. Let $y_t$ be explained by a vector varible $x_t$, such that
\begin{equation}\label{eq: model}
y_t=\beta x_{t-1}+\varepsilon_t,\quad t=1,\cdots,n
\end{equation}
where $x_t$ is a $k\times 1$ vector of stochastic variables and $\beta\in \mathbb{R}^k$ is a $k\times 1$ vector of unknown parameters. Furthermore, following, define the information vector $\mathcal{I}_{t}=[x_0',\cdots,x_t']$ defined for $t=1,\cdots,T-1$, with the convention that $\mathcal{I}_0=x_0$.
\item{}. Suppose we wish to test the null hypothesis
\[
H_0:\beta=0
\] 
against the alternative
\[
H_1: \beta=\beta_1
\]
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item {} To build their tests assume that the error terms has a conditional zero median, such that 
\begin{equation}\label{eq: errors}
P[\varepsilon_t\geq0\mid \mathcal{I}_{t-1}]=P[\varepsilon_t<0\mid \mathcal{I}_{t-1}]
\end{equation}
\item{} Tests derived under (\ref{eq: errors}) are interpreted as tests of whether the conditional median of $y_t$ is predictable using $x_{t-1}$. 
\item{} To build our POS-bases tests, we follow by further assuming that the error terms follow a mediangale process conditional on $\mathcal{I}_{t-1}$
\[
P[\varepsilon_t\geq 0 \mid \varepsilon_1,\cdots,\varepsilon_{t-1}, \mathcal{I}_{t-1}]=P[\varepsilon_t< 0 \mid \varepsilon_1,\cdots,\varepsilon_{t-1}, \mathcal{I}_{t-1}]=\frac{1}{2} 
\]
\item{} This assumption allows for serial (non-linear) dependence between the error terms.
\end{itemize}
\end{frame}

\begin{frame}
\begin{block}{Theorem}
Under the assumption (\ref{eq: errors}) and given the model (\ref{eq: model}), the signs $s(\varepsilon_1),\cdots,s(\varepsilon_n)$ are i.i.d conditional on $\mathcal{I}_{t-1}$, according to the distribution
\[
P[s(\varepsilon_t)=1\mid \mathcal{I}_{t-1}]=P[s(\varepsilon_t)=0\mid \mathcal{I}_{t-1}]=\frac{1}{2},\quad t=1,\cdots,n
\]
\end{block}
\end{frame}

\begin{frame}
\begin{itemize}
\item{} We define the vector of signs $U(n)=(s(y_1),\cdots,s(y_n))'$, where $s(y_t)=\mathbbm{1}_{\mathbb{R}^+\cup 0}\{y_t\}$. Thus, the likelihood function of the sample in terms of signs under the null hypothesis is
\begin{eqnarray*}
L(U(n),0)&=&\prod\limits_{t=1}^{n}P[s(y_t)=s_t\mid \mathcal{I}_{t-1}]\\
&=&\prod\limits_{t=1}^{n}P[y_t\geq 0\mid \mathcal{I}_{t-1}]^{s_t}P[y_t< 0\mid \mathcal{I}_{t-1}]^{1-s_t}\\
&=&\prod\limits_{t=1}^{n}P[\varepsilon_t\geq 0\mid \mathcal{I}_{t-1}]^{s_t}P[\varepsilon_t< 0\mid \mathcal{I}_{t-1}]^{1-s_t}\\
&=&\left(\frac{1}{2}\right)^n
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item{} Under the alternative, the signs $s(y_1),\cdots,s(y_n)$ are no longer independent
\begin{eqnarray*}
L(U(n),\beta_1)&=&\prod\limits_{t=1}^{n}P[s(y_t)=s_t\mid \bm{S}_{t-1},\mathcal{I}_{t-1}]\\
&=&\prod\limits_{t=1}^{n}P[y_t\geq 0\mid \bm{S}_{t-1},\mathcal{I}_{t-1}]^{s(y_t)}\times\\
&&P[y_t< 0\mid \bm{S}_{t-1},\mathcal{I}_{t-1}]^{1-s(y_t)}
\end{eqnarray*}
\item{} where $\bm{S}_{t-1}=\{s(y_{t-1})=s_{t-1},\cdots,s(y_1)=s_1\}$ with $\bm{S}_0=\{\emptyset\}$ and $P[s(y_1)=s_1\mid \bm{S}_0]=P[s(y_1)=s_1]$.
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item{} We may assume that process of signs $\{s(y_t)\}_{t=0}^{\infty}$ follows a first order Markov process - i.e.
\begin{align*}
\begin{array}{l}
P[y_t\geq 0\mid \bm{S}_{t-1},\mathcal{I}_{t-1}]=\\
P[y_t\geq0\mid y_{t-1}\geq0,\mathcal{I}_{t-1}]^{s(y_{t-1})}P[y_t\geq0\mid y_{t-1}<0, \mathcal{I}_{t-1}]^{1-s(y_{t-1})}\\
P[y_t< 0\mid \bm{S}_{t-1},\mathcal{I}_{t-1}]=\\
P[y_t<0\mid y_{t-1}\geq0,\mathcal{I}_{t-1}]^{s(y_{t-1})}P[y_t<0\mid y_{t-1}<0,\mathcal{I}_{t-1}]^{1-s(y_{t-1})}
\end{array}
\end{align*}
\item{} Thus, it can be shown that the test statistic is given by
\begin{equation*}
\tilde{S}L_{n}(\beta _{1})=\sum\limits_{t=1}^{n}\tilde{a}_{t}(\beta _{1})%
s(y_{t})+\sum\limits_{t=1}^{n}\tilde{b}_{t}(\beta _{1})%
s(y_{t})s(y_{t-1}),
\end{equation*}%
where%
\begin{equation*}
\tilde{a}_{1}(\beta _{1})=\ln \left\{ \frac{1-P\left[ \varepsilon_{1}<-\beta
_{1}'x_{0}\mid\mathcal{I}_{0}\right] }{P\left[ \varepsilon_{1}<-\beta _{1}^{^{\prime
}}x_{0}\mid\mathcal{I}_{0}\right] }\right\} ,\quad\tilde{b}_{1}(\beta _{1})=0
\end{equation*}%
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item{} and for $t=2,...,n$,%
\begin{equation*}
\begin{tabular}{l}
$\tilde{a}_{t}(\beta _{1})=\ln \left\{ \frac{1-\frac{P\left[
\varepsilon_{t}<-\beta_1'x_{t-1},\text{ }\varepsilon_{t-1}<-\beta _{1}'
x_{t-2}\mid\mathcal{I}_{t-1}\right] }{P\left[ \varepsilon_{t-1}<-\beta _{1}'
x_{t-2}\mid\mathcal{I}_{t-1}\right] }}{\frac{P\left[ \varepsilon_{t}<-\beta _{1}'
x_{t-1},\text{ }\varepsilon_{t-1}<-\beta_1'x_{t-2}\mid\mathcal{I}_{t-1}\right] }{P%
\left[ \varepsilon_{t-1}<-\beta_1'x_{t-2}\mid\mathcal{I}_{t-1}\right] }}\right\} $ \\ 
\\ 
$\tilde{b}_{t}(\beta _{1})=\ln \left\{ \frac{1-\left( \frac{P\left[
\varepsilon_{t}<-\beta_1'x_{t-1}\mid\mathcal{I}_{t-1}\right] }{1-P\left[
\varepsilon_{t-1}<-\beta_1'x_{t-2}\mid\mathcal{I}_{t-1}\right] }-\frac{P\left[ 
\text{ }\varepsilon_{t-1}<-\beta_1'x_{t-2},\text{ }\varepsilon_{t}<-\beta
_{1}'x_{t-1}\mid\mathcal{I}_{t-1}\right] }{1-P\left[ \varepsilon_{t-1}<-\beta
_{1}'x_{t-2}\mid\mathcal{I}_{t-1}\right] }\right) }{\frac{P\left[
\varepsilon_{t}<-\beta_1'x_{t-1}\mid\mathcal{I}_{t-1}\right] }{1-P\left[
\varepsilon_{t-1}<-\beta_1'x_{t-2}\mid\mathcal{I}_{t-1}\right] }-\frac{P\left[ 
\text{ }\varepsilon_{t-1}<-\beta_1'x_{t-2},\text{ }\varepsilon_{t}<-\beta
_{1}'x_{t-1}\mid\mathcal{I}_{t-1}\right] }{1-P\left[ \varepsilon_{t-1}<-\beta
_{1}'x_{t-2}\mid\mathcal{I}_{t-1}\right] }}\right\} $\\
$\textcolor{white}{\tilde{b}_{t}(\beta _{1})=}-\ln \left\{ \frac{1-\frac{%
P\left[ \varepsilon_{t}<-\beta_1'x_{t-1},\text{ }%
\varepsilon_{t-1}<-\beta_1'x_{t-2}\mid\mathcal{I}_{t-1}\right] }{P\left[
\varepsilon_{t-1}<-\beta_1'x_{t-2}\mid\mathcal{I}_{t-1}\right] }}{\frac{P\left[
\varepsilon_{t}<-\beta_1'x_{t-1},\text{ }\varepsilon_{t-1}<-\beta _{1}'
x_{t-2}\mid\mathcal{I}_{t-1}\right] }{P\left[ \varepsilon_{t-1}<-\beta _{1}'
x_{t-2}\mid\mathcal{I}_{t-1}\right] }}\right\} $%
\end{tabular}%
\end{equation*}%
\end{itemize}
\end{frame}

\begin{frame}{The problem}
\begin{block}{The weights}
The problem consists of evaluation the univariate and bivariate probabilities
\[
P\left[ \varepsilon_{t}<-\beta_1'x_{t-1},\text{ }%
\varepsilon_{t-1}<-\beta_1'x_{t-2}<0\mid\mathcal{I}_{t-1} \right]
\]
and
\[
\quad P\left[ \varepsilon_{t}<-\beta_1'x_{t-1}\mid\mathcal{I}_{t-1}\right]
\]
for $t=1,\cdots,T$
\end{block}
\end{frame}
\begin{frame}{Dependence modelling using copulas}
\begin{itemize}
\item {} As the mediangale assumption allows for non-linear serial dependence, testing assumption A1 by considering linear correlation is inappropriate. 

\item{} we suggest fitting copula models, which provide the means of separating the marginal distribution of the process from their respective dependence structure.
\item{} For instance, the marginals can be assumed to possess standard normal distributions, while the nonlinear dependency is modeled using rank correlation measures (e.g. Kendall Tau) and copulas, where the latter are invariant under monotonic transformations; hence, they are not affected by the marginal distributions [see ].
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item{}A special case is where $\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_{T-1},\varepsilon_T$ are distributed according to $N(0,1)$. As suggested before, since the form of the serial dependence of the errors is non-linear, we may estimate the bivariate probabilities using copulas. 

\item{}The latter stem from a Theorem brought forward by, which states that there is a copula $C$ such that
\[
F_{\varepsilon}(-\beta' x_0,\cdots,-\beta' x_{n-1})=C(F_1(-\beta' x_0),\cdots,F_n(-\beta' x_{n-1})).
\]
\item{}Therefore, as noted earlier, the bivariate probabilities can be calculated using copulas, which separate the marginal distribution of the error terms from their dependence structure. We may consider a rank correlation measure (e.g. Kendall Tau) as the measure of non-linear dependency and choose the Gaussian copula for evaluating the bivariate probabilities $P\left[ \text{ }\varepsilon_{t-1}<\cdot ,\-
\text{ }\varepsilon_{t}<\cdot \mid \mathcal{I}_{t-1}\right]$.
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item{} First let us forget about the Theorem mentioned earlier, where $s(\varepsilon_1),\cdots, s(\varepsilon_n)$ are i.i.d conditional on $\mathcal{I}_{t-1}$. Let us explore this from an earlier point and what led to this Theorem in . We know from the mediangale assumption that
\begin{align*}
\begin{array}{l}
P[s(\varepsilon_t)=s_t\mid s(\varepsilon_{t-1})=s_{t-1},\cdots,s(\varepsilon_{1})=s_{1}]=\\
P[\varepsilon_t\geq0\mid \varepsilon_{t-1},\cdots,\varepsilon_{1}]^{s(\varepsilon_t)}
P[\varepsilon_t<0\mid \varepsilon_{t-1},\cdots,\varepsilon_{1}]^{1-s(\varepsilon_t)}=\frac{1}{2}
\end{array}
\end{align*}
\item{} As this corresponds only to the median, it is only true \textit{iff} both sides of the inequality are zero. SInce the probabilities $P[\varepsilon_t\geq0\mid \varepsilon_{t-1},\cdots,\varepsilon_{1}]$ and $P[\varepsilon_t<0\mid \varepsilon_{t-1},\cdots,\varepsilon_{1}]$ are the same and equal to $1/2$ for all $t=1,\cdots,n$. Thus, it can be argued that $s(\varepsilon_1),\cdots,s(\varepsilon_n)$ which are defined by the aformentioned probabilties can be considered i.i.d, as their distribution is determined by the said probabilities, which are identical over time.
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item{} And here is the proposition (pay careful attention to the last paragraph):
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item{} Now let us instead assume that we are interested in the signs $\tilde{s}(\varepsilon+c_t)$, where $c_t$ for $t=1,\cdots,n$ are some constants. Then the likelihood function in terms of the signs $\tilde{s}_1,\cdots,\tilde{s}_n$ 
\begin{align*}
\begin{array}{l}
L(\tilde{s}_1,\cdots,\tilde{s}_n\mid X)=\\
\prod\limits_{t=1}^{n}P[\varepsilon_t\geq -c_t\mid \varepsilon_1,\cdots,\varepsilon_n]^{\tilde{s}_t}P[\varepsilon_t< -c_t \mid \varepsilon_1,\cdots,\varepsilon_n]^{1-\tilde{s}_t}	
\end{array}
\end{align*}
\item{} Now the joint p.m.f. depnends on all the past signs and and thus is no longer $i.i.d$, as it violates the Mediangale assumption (particularly the point about permutations mentioned in Proposition 1 of). In other words, we no longer have that the p.m.fs $P[\varepsilon_t<-c_t\mid \varepsilon_{t-1},\cdots,\varepsilon_{1}]$ are constant and identical over time, so the distribution of the signs $\tilde{s}(\varepsilon_1+c_1),\cdots,\tilde{s}(\varepsilon_n+c_n)$, now dependens on the joint p.m.fs.
\end{itemize}
\end{frame}

\begin{frame}{Alternative proof}
\begin{itemize}
\item{}Under the null hypotheis, it is evident that
\[
P[\varepsilon_t\geq0\mid X]=P[\varepsilon_t<0\mid X],\quad t=1,\cdots,n
\]
is equivalent to
\[
P[y_t\geq0\mid X]=P[y_t<0\mid X],\quad t=1,\cdots,n
\]
\item{}Therefore, the mediangale assumption can be extended to $y_1,\cdots,y_n$:
\[
P[y_t\geq0\mid y_1,\cdots,y_n,X]=P[y_t<0\mid y_1,\cdots,y_n,X]=\frac{1}{2}
\]
\end{itemize}
\end{frame}

\begin{frame}{Alternative proof}
\begin{itemize}
\item{}Then the variables $s(y_1),\cdots,s(y_n)$ are i.i.d conditional on $X$ according to the distribution
\[
P[s(y_t)=1\mid X]=P[s(y_t)=0\mid X]=\frac{1}{2},\quad t=1,\cdots,n
\]
\item{} Under the alternative hypothesis, however, the mediangale property does not hold for permutations $\pi:i\rightarrow j$ as noted in the Proposition 3.1 of , as the conditional distribution of the signs vary across observations.

\item{} Therefore, the signs $s(u_1),\cdots,s(y_n)$ can no longer be assumed to be i.i.d.
\end{itemize}
\end{frame}

%\bibliographystyle{apa}
%\bibliography{References_Final}
\end{document}
