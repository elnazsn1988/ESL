\documentclass{beamer}
\usetheme{Madrid}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[document]{ragged2e}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{float}
\usepackage{subfigure}
\usepackage[round]{natbib}

\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}

\title{Elements of statistical learning:
Chapter 3}
\centering
\begin{document}
\maketitle

\begin{frame}{Content}
\tableofcontents
\end{frame}

\section{Linear models}

\begin{frame}{LS estimator}
Let $\bm{X}$ be an $N\times(p+1)$ matrix of explanatory variables and $\bm{y}$ an $N\times 1$ vector of outputs. Then we know the LS estimator $\hat{\beta}$
\[
\hat{\beta}=(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{y},
\]
 [see lecture slides "ESL1" for recap and proof]. 
\begin{block}{The "hat" matrix}
As such for the fitted linear model
\begin{eqnarray*}
\hat{y}&=&\bm{X}\hat{\beta}\\
&=&\underbrace{\bm{X}(\bm{X}'\bm{X})^{-1}\bm{X}'}_{\bm{H}}\bm{y}\\
&=&\bm{H}\bm{y}
\end{eqnarray*}
where $\bm{H}$ is commonly referred to as the hat matrix.
\end{block}
\end{frame}

\begin{frame}{$\bm{H}$ the projection matrix}
Let us denote the column vectors of $\bm{X}$ by $\bm{x}_0,\bm{x}_1,\cdots,\bm{x}_p$ with $\bm{x}_0\equiv 1$.
\begin{itemize}
\item{} These vectors span a subspace of $\R^N$, also referred to as a column vector of $\bm{X}$.
\item{} We minimize $RSS(\beta)=\lvert\lvert \bm{y}-\bm{X}\beta\rvert\rvert^2$ by choosing $\hat{\beta}$, so that the residual vector $\bm{y}-\hat{\bm{y}}$ is orthogonal to this subspace.
\item{} the hat matrix $\bm{H}$ computes the orthogonal projection, and hence it is also known as the projection matrix.
\end{itemize}
\end{frame}

\subsection{Sampling properties of $\hat{\beta}$}
\begin{frame}{Variance-covariance matrix}
\begin{block}{Assumptions}
\begin{enumerate}
\item{} Observations $y_i$ are uncorrelated have constant variance $\sigma^2$
\item{} $x_i$ are fixed (i.e. non-stochastic)
\end{enumerate}
\end{block}
\begin{eqnarray*}
var(\hat{\beta})&=&var\left[(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{y}\right]\\
&=&var\left[(\bm{X}'\bm{X})^{-1}\bm{X}'(\bm{X}\beta+\epsilon)\right]\\
&=&var\left[(\bm{X}'\bm{X})^{-1}(\bm{X}'\bm{X})\beta+(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{\epsilon}\right]\\
&=&var\left[(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{\epsilon}\right]\\
&=&\E\left\{(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{\epsilon}[(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{\epsilon}]'\right\}\\
&=&\E\left\{(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{\epsilon}\bm{\epsilon}'\bm{X}(\bm{X}'\bm{X})^{-1}\right\}\\
&=&\E\left\{(\bm{X}'\bm{X})^{-1}(\bm{X}'\bm{X})\bm{\epsilon}\bm{\epsilon}'(\bm{X}'\bm{X})^{-1}\right\}
\end{eqnarray*}
\end{frame}

\begin{frame}
Note that $\epsilon$ is the error term and has zero mean and also remember that $\bm{X}$ is fixed, and thus 
\[
\E[aZ]=a\E[Z]
\]
where $Z$ is a random variable and $a$ is a constant. Therefore, 
\begin{eqnarray*}
var(\hat{\beta})&=&\E\left\{\bm{\epsilon}\bm{\epsilon}'(\bm{X}'\bm{X})^{-1}\right\}\\
&=&(\bm{X}'\bm{X})^{-1}E\left\{\bm{\epsilon}\bm{\epsilon}'\right\}\\
&=&(\bm{X}'\bm{X})^{-1}\sigma^2
\end{eqnarray*}
where $\sigma^2$ can be calculated by
\[
\sigma^2=\frac{1}{N-p-1}\sum\limits_{i=1}^N(y_i-\hat{y}_i)^2
\]
thus, assuming the errors are further Gaussian
\[
\hat{\beta}\sim N(\beta,(\bm{X}'\bm{X})^{-1}\sigma^2)
\]
\end{frame}
\subsection{Gauss-Markov Theorem}
\begin{frame}{Gauss-Markov Theorem}
Least squares estimator of parameter $\beta$ has the smallest variance among all linear unbiased estimators.
Why is the LS estimator unbiased?
\begin{proof}
\begin{eqnarray*}
\hat{\beta}&=&\E[\hat{\beta}]\\
&=&\E[(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{y}]\\
&=&\E[(\bm{X}'\bm{X})^{-1}\bm{X}'(\bm{X}\beta+\bm{\epsilon})]\\
&=&\E[\beta+(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{\epsilon}]\\
&=&\beta+(\bm{X}'\bm{X})^{-1}\bm{X}'\E[\bm{\epsilon}]\\
&=&\beta
\end{eqnarray*}
\end{proof}
\end{frame}

\section{Multiple regression}
\subsection{From simple univariate to multiple regressions}
\begin{frame}{From simple univariate to muliple regressions}
Suppose first we have a univariate model with no intercept
\[
Y_i=X_i\beta+\varepsilon_i
\]
The least squares estimates and residuals are
\[
\hat{\beta}=\frac{\sum\limits_{i=1}^{N}x_iy_i}{\sum\limits_{i=1}^{N}x_i^2}
\]
with residuals
\[
r_i=y_i-x_i\hat{\beta}
\]
which in vector notation can be expressed as 
\[
<\bm{x},\bm{y}>=\sum\limits_{i=1}^{N}x_iy_i=\bm{x}'\bm{y}
\] 
which is the inner product between $\bm{x}$ and $\bm{y}$.
\end{frame}

\begin{frame}
Thus, the OLS estimator $\hat{\beta}$ can be expressed as follows
\[
\hat{\beta}=\frac{<\bm{x},\bm{y}>}{<\bm{x},\bm{x}>},
\]
Suppose now that we have $p$ inputs $\bm{x}_1,\cdots,\bm{x}_p$, which are the columns of the matrix $\bm{X}$ and are orthogonal, such that $<\bm{x}_j,\bm{x}_k>=0$ for all $i\neq j$. When the inputs are orthogonal, the multiple least squares estimates $\hat{\beta}_j$ are equal tothe univariate estimates - i.e. 
\[
\hat{\beta}_j=\frac{<\bm{x}_j,\bm{y}>}{<\bm{x}_j,\bm{x}_j>}
\] 
In other words, the inputs are orthogonal and have no impact on each other's parameters estimates in the model.
\end{frame}

\begin{frame}
Consider the case of an intercept and a single input $\bm{x}$, then the least squares coefficient of $\bm{x}$ has the form
\[
\hat{\beta}_1=\frac{<\bm{x}-\bar{x}\bm{1},\bm{y}>}{<\bm{x}-\bar{x}\bm{1},\bm{x}-\bar{x}\bm{1}>}
\]
The steps of the algorithm can be seen as follows
\begin{enumerate}
\item{} Regress $\bm{x}$ on $\bm{1}$ to obtain $\bar{x}\bm{1}$

\item{} Obtain the residuals $\bm{z}=\bm{x}-\bar{x}\bm{1}$

\item{} Regress $\bm{y}$ on $\bm{z}$ to obtain the coefficient $\hat{\beta}_1$
\end{enumerate}
\[
\hat{\beta}_1=\frac{<\bm{z},\bm{y}>}{<\bm{z},\bm{z}>}
\]
Step 1 orthogonalizes $x$ with respect to $x_o=1$.  
\end{frame}


%\bibliographystyle{apa}
%\bibliography{References_Final}
\end{document}
